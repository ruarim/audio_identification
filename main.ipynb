{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage.feature import peak_local_max\n",
    "import pickle\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pop.00027 - 1 of 200 documents processed\n",
      "pop.00033 - 2 of 200 documents processed\n",
      "classical.00079 - 3 of 200 documents processed\n",
      "classical.00045 - 4 of 200 documents processed\n",
      "classical.00051 - 5 of 200 documents processed\n",
      "classical.00086 - 6 of 200 documents processed\n",
      "classical.00092 - 7 of 200 documents processed\n",
      "classical.00093 - 8 of 200 documents processed\n",
      "classical.00087 - 9 of 200 documents processed\n",
      "classical.00050 - 10 of 200 documents processed\n",
      "classical.00044 - 11 of 200 documents processed\n",
      "classical.00078 - 12 of 200 documents processed\n",
      "pop.00032 - 13 of 200 documents processed\n",
      "pop.00026 - 14 of 200 documents processed\n",
      "pop.00030 - 15 of 200 documents processed\n",
      "pop.00024 - 16 of 200 documents processed\n",
      "pop.00018 - 17 of 200 documents processed\n",
      "classical.00052 - 18 of 200 documents processed\n",
      "classical.00046 - 19 of 200 documents processed\n",
      "classical.00091 - 20 of 200 documents processed\n",
      "classical.00085 - 21 of 200 documents processed\n",
      "classical.00084 - 22 of 200 documents processed\n",
      "classical.00090 - 23 of 200 documents processed\n",
      "classical.00047 - 24 of 200 documents processed\n",
      "classical.00053 - 25 of 200 documents processed\n",
      "pop.00019 - 26 of 200 documents processed\n",
      "pop.00025 - 27 of 200 documents processed\n",
      "pop.00031 - 28 of 200 documents processed\n",
      "pop.00009 - 29 of 200 documents processed\n",
      "pop.00035 - 30 of 200 documents processed\n",
      "pop.00021 - 31 of 200 documents processed\n",
      "classical.00057 - 32 of 200 documents processed\n",
      "classical.00043 - 33 of 200 documents processed\n",
      "classical.00094 - 34 of 200 documents processed\n",
      "classical.00080 - 35 of 200 documents processed\n",
      "classical.00081 - 36 of 200 documents processed\n",
      "classical.00095 - 37 of 200 documents processed\n",
      "classical.00042 - 38 of 200 documents processed\n",
      "classical.00056 - 39 of 200 documents processed\n",
      "pop.00020 - 40 of 200 documents processed\n",
      "pop.00034 - 41 of 200 documents processed\n",
      "pop.00008 - 42 of 200 documents processed\n",
      "pop.00022 - 43 of 200 documents processed\n",
      "pop.00036 - 44 of 200 documents processed\n",
      "classical.00040 - 45 of 200 documents processed\n",
      "classical.00054 - 46 of 200 documents processed\n",
      "classical.00068 - 47 of 200 documents processed\n",
      "classical.00083 - 48 of 200 documents processed\n",
      "classical.00097 - 49 of 200 documents processed\n",
      "classical.00096 - 50 of 200 documents processed\n",
      "classical.00082 - 51 of 200 documents processed\n",
      "classical.00069 - 52 of 200 documents processed\n",
      "classical.00055 - 53 of 200 documents processed\n",
      "classical.00041 - 54 of 200 documents processed\n",
      "pop.00037 - 55 of 200 documents processed\n",
      "pop.00023 - 56 of 200 documents processed\n",
      "pop.00044 - 57 of 200 documents processed\n",
      "pop.00050 - 58 of 200 documents processed\n",
      "pop.00078 - 59 of 200 documents processed\n",
      "pop.00087 - 60 of 200 documents processed\n",
      "pop.00093 - 61 of 200 documents processed\n",
      "classical.00026 - 62 of 200 documents processed\n",
      "classical.00032 - 63 of 200 documents processed\n",
      "classical.00033 - 64 of 200 documents processed\n",
      "classical.00027 - 65 of 200 documents processed\n",
      "pop.00092 - 66 of 200 documents processed\n",
      "pop.00086 - 67 of 200 documents processed\n",
      "pop.00079 - 68 of 200 documents processed\n",
      "pop.00051 - 69 of 200 documents processed\n",
      "pop.00045 - 70 of 200 documents processed\n",
      "pop.00053 - 71 of 200 documents processed\n",
      "pop.00047 - 72 of 200 documents processed\n",
      "pop.00090 - 73 of 200 documents processed\n",
      "pop.00084 - 74 of 200 documents processed\n",
      "classical.00019 - 75 of 200 documents processed\n",
      "classical.00031 - 76 of 200 documents processed\n",
      "classical.00025 - 77 of 200 documents processed\n",
      "classical.00024 - 78 of 200 documents processed\n",
      "classical.00030 - 79 of 200 documents processed\n",
      "classical.00018 - 80 of 200 documents processed\n",
      "pop.00085 - 81 of 200 documents processed\n",
      "pop.00091 - 82 of 200 documents processed\n",
      "pop.00046 - 83 of 200 documents processed\n",
      "pop.00052 - 84 of 200 documents processed\n",
      "pop.00056 - 85 of 200 documents processed\n",
      "pop.00042 - 86 of 200 documents processed\n",
      "pop.00095 - 87 of 200 documents processed\n",
      "pop.00081 - 88 of 200 documents processed\n",
      "classical.00034 - 89 of 200 documents processed\n",
      "classical.00020 - 90 of 200 documents processed\n",
      "classical.00008 - 91 of 200 documents processed\n",
      "classical.00009 - 92 of 200 documents processed\n",
      "classical.00021 - 93 of 200 documents processed\n",
      "classical.00035 - 94 of 200 documents processed\n",
      "pop.00080 - 95 of 200 documents processed\n",
      "pop.00094 - 96 of 200 documents processed\n",
      "pop.00043 - 97 of 200 documents processed\n",
      "pop.00057 - 98 of 200 documents processed\n",
      "pop.00069 - 99 of 200 documents processed\n",
      "pop.00041 - 100 of 200 documents processed\n",
      "pop.00055 - 101 of 200 documents processed\n",
      "pop.00082 - 102 of 200 documents processed\n",
      "pop.00096 - 103 of 200 documents processed\n",
      "classical.00023 - 104 of 200 documents processed\n",
      "classical.00037 - 105 of 200 documents processed\n",
      "classical.00036 - 106 of 200 documents processed\n",
      "classical.00022 - 107 of 200 documents processed\n",
      "pop.00097 - 108 of 200 documents processed\n",
      "pop.00083 - 109 of 200 documents processed\n",
      "pop.00054 - 110 of 200 documents processed\n",
      "pop.00040 - 111 of 200 documents processed\n",
      "pop.00068 - 112 of 200 documents processed\n",
      "pop.00065 - 113 of 200 documents processed\n",
      "pop.00071 - 114 of 200 documents processed\n",
      "pop.00059 - 115 of 200 documents processed\n",
      "classical.00007 - 116 of 200 documents processed\n",
      "classical.00013 - 117 of 200 documents processed\n",
      "classical.00012 - 118 of 200 documents processed\n",
      "classical.00006 - 119 of 200 documents processed\n",
      "pop.00058 - 120 of 200 documents processed\n",
      "pop.00070 - 121 of 200 documents processed\n",
      "pop.00064 - 122 of 200 documents processed\n",
      "pop.00072 - 123 of 200 documents processed\n",
      "pop.00066 - 124 of 200 documents processed\n",
      "pop.00099 - 125 of 200 documents processed\n",
      "classical.00038 - 126 of 200 documents processed\n",
      "classical.00010 - 127 of 200 documents processed\n",
      "classical.00004 - 128 of 200 documents processed\n",
      "classical.00005 - 129 of 200 documents processed\n",
      "classical.00011 - 130 of 200 documents processed\n",
      "classical.00039 - 131 of 200 documents processed\n",
      "pop.00098 - 132 of 200 documents processed\n",
      "pop.00067 - 133 of 200 documents processed\n",
      "pop.00073 - 134 of 200 documents processed\n",
      "pop.00077 - 135 of 200 documents processed\n",
      "pop.00063 - 136 of 200 documents processed\n",
      "pop.00088 - 137 of 200 documents processed\n",
      "classical.00015 - 138 of 200 documents processed\n",
      "classical.00001 - 139 of 200 documents processed\n",
      "classical.00029 - 140 of 200 documents processed\n",
      "classical.00028 - 141 of 200 documents processed\n",
      "classical.00000 - 142 of 200 documents processed\n",
      "classical.00014 - 143 of 200 documents processed\n",
      "pop.00089 - 144 of 200 documents processed\n",
      "pop.00062 - 145 of 200 documents processed\n",
      "pop.00076 - 146 of 200 documents processed\n",
      "pop.00048 - 147 of 200 documents processed\n",
      "pop.00060 - 148 of 200 documents processed\n",
      "pop.00074 - 149 of 200 documents processed\n",
      "classical.00002 - 150 of 200 documents processed\n",
      "classical.00016 - 151 of 200 documents processed\n",
      "classical.00017 - 152 of 200 documents processed\n",
      "classical.00003 - 153 of 200 documents processed\n",
      "pop.00075 - 154 of 200 documents processed\n",
      "pop.00061 - 155 of 200 documents processed\n",
      "pop.00049 - 156 of 200 documents processed\n",
      "pop.00006 - 157 of 200 documents processed\n",
      "pop.00012 - 158 of 200 documents processed\n",
      "classical.00058 - 159 of 200 documents processed\n",
      "classical.00064 - 160 of 200 documents processed\n",
      "classical.00070 - 161 of 200 documents processed\n",
      "classical.00071 - 162 of 200 documents processed\n",
      "classical.00065 - 163 of 200 documents processed\n",
      "classical.00059 - 164 of 200 documents processed\n",
      "pop.00013 - 165 of 200 documents processed\n",
      "pop.00007 - 166 of 200 documents processed\n",
      "pop.00011 - 167 of 200 documents processed\n",
      "pop.00005 - 168 of 200 documents processed\n",
      "pop.00039 - 169 of 200 documents processed\n",
      "classical.00073 - 170 of 200 documents processed\n",
      "classical.00067 - 171 of 200 documents processed\n",
      "classical.00098 - 172 of 200 documents processed\n",
      "classical.00099 - 173 of 200 documents processed\n",
      "classical.00066 - 174 of 200 documents processed\n",
      "classical.00072 - 175 of 200 documents processed\n",
      "pop.00038 - 176 of 200 documents processed\n",
      "pop.00004 - 177 of 200 documents processed\n",
      "pop.00010 - 178 of 200 documents processed\n",
      "pop.00028 - 179 of 200 documents processed\n",
      "pop.00014 - 180 of 200 documents processed\n",
      "pop.00000 - 181 of 200 documents processed\n",
      "classical.00076 - 182 of 200 documents processed\n",
      "classical.00062 - 183 of 200 documents processed\n",
      "classical.00089 - 184 of 200 documents processed\n",
      "classical.00088 - 185 of 200 documents processed\n",
      "classical.00063 - 186 of 200 documents processed\n",
      "classical.00077 - 187 of 200 documents processed\n",
      "pop.00001 - 188 of 200 documents processed\n",
      "pop.00015 - 189 of 200 documents processed\n",
      "pop.00029 - 190 of 200 documents processed\n",
      "pop.00003 - 191 of 200 documents processed\n",
      "pop.00017 - 192 of 200 documents processed\n",
      "classical.00061 - 193 of 200 documents processed\n",
      "classical.00075 - 194 of 200 documents processed\n",
      "classical.00049 - 195 of 200 documents processed\n",
      "classical.00048 - 196 of 200 documents processed\n",
      "classical.00074 - 197 of 200 documents processed\n",
      "classical.00060 - 198 of 200 documents processed\n",
      "pop.00016 - 199 of 200 documents processed\n",
      "pop.00002 - 200 of 200 documents processed\n",
      "---number of hashes---\n",
      "---time to run fingerprinting 361.68797820201144 seconds---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# helper function to dump all database items to a file\n",
    "def dump(data, dir, name):\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "    \n",
    "    #create the file path\n",
    "    file_path = os.path.join(dir, f\"{name}.pkl\")\n",
    "    \n",
    "    # write the data to the file\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(data, file, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def spectral_peaks(y, window_hop=5, threshhold=0.05):    \n",
    "    # take STFT and get magnitude values\n",
    "    stft = np.abs(librosa.stft(y))\n",
    "    # get the peaks           \n",
    "    peaks = peak_local_max(stft, min_distance=window_hop, threshold_abs=threshhold)\n",
    "    return peaks # index of peaks\n",
    "\n",
    "# use every point in constelation map as anchor then find pair in target zone        \n",
    "def peak_combinations(peaks, id, target_size=32, fan_max=10):\n",
    "    peaks = np.array(peaks) # use numpy for brevity\n",
    "    \n",
    "    combinations = {}\n",
    "    # set each peak as an anchor\n",
    "    for anchor in peaks: \n",
    "        # define the current target bounds\n",
    "        upper_bound = anchor + target_size\n",
    "        # find peaks in target range\n",
    "        neighbours = peaks[(peaks > anchor)     .all(axis=1) & # this is slow - instead directly index the constelation map with window\n",
    "                           (peaks < upper_bound).all(axis=1)] \n",
    "        \n",
    "        # combine anchor with each neighbour and time diff\n",
    "        for i in range(len(neighbours)): # create get neighbours function\n",
    "            # value \n",
    "            time_stamp = anchor[1]\n",
    "            data       = {\"id\": id, \"time_stamp\": time_stamp} # object containing id and offset timestamp\n",
    "            # key\n",
    "            time_diff  = neighbours[i][1] - time_stamp\n",
    "            hash       = (anchor[0], neighbours[i][0], time_diff) # create hash tuple\n",
    "            \n",
    "            # create key value pair\n",
    "            combinations[hash] = [data] # add new key\n",
    "            \n",
    "            if i == fan_max: \n",
    "                break\n",
    "    \n",
    "    return combinations\n",
    "\n",
    "def fingerprint_file(file, id=None, resample=16384):\n",
    "    y, sr    = librosa.load(file, sr=resample)\n",
    "    peaks    = spectral_peaks(y)\n",
    "    features = peak_combinations(peaks, id)    \n",
    "    return features\n",
    "\n",
    "# helper function to get id based on GTZAN dataset file names\n",
    "def get_id(file_name):\n",
    "    split = file_name.split('.')\n",
    "    id = split[0] + '.' + split[1][:5]\n",
    "    return id\n",
    "\n",
    "def fingerprintBuilder(db_path, fingerprints_path, dataset_size=200, show=False):\n",
    "    doc_count = 0\n",
    "    num_documents = len(os.listdir(db_path))\n",
    "    fingerprints = {}\n",
    "    \n",
    "    start_time = time.perf_counter()\n",
    "    for entry in os.scandir(db_path):\n",
    "        id = get_id(entry.name)\n",
    "        combinations = fingerprint_file(entry, id=id) # fingerprint\n",
    "        \n",
    "        # add combinations to fingerprints data structure\n",
    "        for hash in combinations.keys():\n",
    "            value = combinations[hash][0] # this is not ideal but it works\n",
    "            if hash in fingerprints: fingerprints[hash].append(value) # if the a hash exists append to current array\n",
    "            else: fingerprints[hash] = [value] # add new key\n",
    "                \n",
    "        doc_count+=1\n",
    "        if show:\n",
    "            print(\"{} - {} of {} documents processed\".format(id, doc_count, num_documents))\n",
    "        if doc_count == dataset_size: break\n",
    "        \n",
    "    # show run time and hash count\n",
    "    end_time = time.perf_counter()\n",
    "    total_time = end_time - start_time\n",
    "    print(\"---number of hashes {}---\".format(len(fingerprints)))\n",
    "    print(\"---time to run fingerprinting {} seconds---\".format(total_time))\n",
    "    \n",
    "    # dump values to file\n",
    "    dump(fingerprints, fingerprints_path, \"documents\")\n",
    "\n",
    "db_path = \"_database_recordings\"\n",
    "fingerprint_path = \"_fingerprints\"\n",
    "fingerprintBuilder(db_path, fingerprint_path, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---time to run identification 157.12603164699976 seconds---\n",
      "---percentage correct 0.7793427230046949---\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "# helpering function for loading pickled data\n",
    "def load(name):\n",
    "    with open(name + '.pkl', 'rb') as file:\n",
    "        loaded_data = pickle.load(file)\n",
    "    return loaded_data\n",
    "\n",
    "def identify(Q, D):    \n",
    "    # calulcate the frequnecy of matching offsets\n",
    "    matches = get_matches(Q, D)\n",
    "    # find documents with most frequnent shifts\n",
    "    candidates = get_largest_matches(matches)\n",
    "    # sort most frequnent ids by count\n",
    "    sorted_candidates = sort_dic(candidates)\n",
    "\n",
    "    return sorted_candidates # [0] = most common\n",
    "\n",
    "def get_matches(Q, D):\n",
    "    matches = defaultdict(Counter) # counter for shift frequency - defaultdict allows arbitrary keys\n",
    "     # find valid shifts   \n",
    "    for h_q in Q: # for each hash index in query\n",
    "        if h_q not in D: continue # only search hash indexes in query\n",
    "        for n in Q[h_q]:\n",
    "            for l in D[h_q]:\n",
    "                m    = l[\"time_stamp\"] - n[\"time_stamp\"]\n",
    "                D_id = l[\"id\"]\n",
    "                matches[D_id][m] += 1\n",
    "                \n",
    "    return matches\n",
    "\n",
    "# create dict of most most common matches for each id - returns dict {\"doc_id\" : count}\n",
    "def get_largest_matches(matches):\n",
    "    candidates = {}\n",
    "    \n",
    "    for D_id, counter in matches.items():\n",
    "        most_common_offset, count = counter.most_common(1)[0] # get most common time shift\n",
    "        candidates[D_id] = count # add to list of possible matches\n",
    "    \n",
    "    return candidates\n",
    "\n",
    "# sort a dict by value - return keys\n",
    "def sort_dic(dict):\n",
    "    values = [key for key in dict]\n",
    "    keys_by_value = sorted(values, key=lambda x: -dict[x])\n",
    "    return keys_by_value\n",
    "\n",
    "def write_output_line(output_file, docs, query_name):\n",
    "    if len(docs) > 0:\n",
    "        output_line = \"%s\\t%s\\n\" % (\n",
    "            query_name,\n",
    "            \"\\t\".join([doc + '.wav' for doc in docs[:min(3, len(docs))]]))\n",
    "    else:\n",
    "        output_line = query_name\n",
    "    output_file.write(output_line)\n",
    "\n",
    "def audioIdentification(query_path, fingerprints_path, output_path):\n",
    "    D = load(fingerprints_path) # load fingerprints\n",
    "    output_file = open(output_path, \"w\")\n",
    "    matches = 0 # keep track of correct matches\n",
    "    count   = 0\n",
    "    start_time = time.perf_counter()\n",
    "    # start profile timer\n",
    "    for entry in os.scandir(query_path):\n",
    "        count+=1\n",
    "        query_name = entry.name\n",
    "        id  = get_id(query_name)\n",
    "        # finger print the query\n",
    "        Q = fingerprint_file(entry, id=id)\n",
    "        # find matching documents\n",
    "        doc_ids = identify(Q, D) \n",
    "        if(len(doc_ids) > 0 and doc_ids[0] == id): matches+=1\n",
    "        # write top documents 3 to file\n",
    "        write_output_line(output_file, doc_ids, query_name)\n",
    "    \n",
    "    # stop profile timer \n",
    "    end_time = time.perf_counter()\n",
    "    # show runtime\n",
    "    total_time = end_time - start_time\n",
    "    print(\"---time to run identification {} seconds---\".format(total_time))\n",
    "    print(\"---percentage correct {}---\".format(matches / count))\n",
    "    \n",
    "    output_file.close()\n",
    "\n",
    "fingerprints_path = '_fingerprints/documents'\n",
    "query_path = \"_query_recordings\"\n",
    "output_path = \"_output.txt\"\n",
    "audioIdentification(query_path, fingerprints_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
